{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"train_sentences.txt\", \"r\")\n",
    "data = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [x.strip() for x in open(\"objects.txt\", \"r\").readlines()]\n",
    "relations = [x.strip() for x in open(\"relations.txt\", \"r\").readlines()]\n",
    "relations = [x.replace(\" \", \"_\") for x in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eros = []\n",
    "for dp in data:\n",
    "    if dp.strip() == \"\":\n",
    "        continue\n",
    "    dp = dp.replace(\" \", \"_\")\n",
    "    ero_list = dp.strip().split(\"\\t\")\n",
    "    if len(ero_list) == 0:\n",
    "        continue\n",
    "        # all_eros.append(ero_list)\n",
    "    all_eros.append(ero_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_points = []\n",
    "for id, dp in enumerate(data):\n",
    "    if dp.strip() == \"\":\n",
    "        continue\n",
    "    dp = dp.replace(\" \", \"_\")\n",
    "    ero_list = dp.strip().split(\"\\t\")\n",
    "    if len(ero_list) <= 1:\n",
    "        continue\n",
    "    for i in range(10): \n",
    "        chosen_index = np.random.randint(0, len(ero_list))\n",
    "        if random.uniform(0,1) < 0.5:\n",
    "            target_ero = ero_list[chosen_index]\n",
    "            context_ero = ero_list[:chosen_index] + ero_list[chosen_index + 1:]\n",
    "            prediction_points.append([context_ero, target_ero, 1])\n",
    "        else:\n",
    "            if random.uniform(0,1) < 0.25:\n",
    "                e = random.sample(objects, 1)[0]\n",
    "                r = random.sample(relations, 1)[0]\n",
    "                o = random.sample(objects, 1)[0]\n",
    "                target_ero = \",\".join([e, r, o])\n",
    "                if (isinstance(target_ero, str) == False):\n",
    "                    print(\"grave error\")\n",
    "                context_ero = ero_list[:chosen_index] + ero_list[chosen_index + 1:]\n",
    "                prediction_points.append([context_ero, target_ero, 0])\n",
    "            else:\n",
    "                # ids = list(range(len(all_eros)))\n",
    "                # ids.remove(id)\n",
    "                # sampled_dp = random.choice(ids)\n",
    "                sampled_dp = np.random.randint(0, len(all_eros))\n",
    "                if sampled_dp == id:\n",
    "                    sampled_dp = np.random.randint(0, len(all_eros))\n",
    "                target_ero = random.sample(all_eros[sampled_dp], 1)[0]\n",
    "                if (target_ero == \"\"):\n",
    "                    print(\"grave error\")\n",
    "                context_ero = ero_list[:chosen_index] + ero_list[chosen_index + 1:]\n",
    "                prediction_points.append([context_ero, target_ero, 0])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_points, open(\"train_preds.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"val_sentences.txt\", \"r\")\n",
    "data = fp.readlines()\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [x.strip() for x in open(\"objects.txt\", \"r\").readlines()]\n",
    "relations = [x.strip() for x in open(\"relations.txt\", \"r\").readlines()]\n",
    "relations = [x.replace(\" \", \"_\") for x in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eros = []\n",
    "for dp in data:\n",
    "    if dp.strip() == \"\":\n",
    "        continue\n",
    "    dp = dp.replace(\" \", \"_\")\n",
    "    ero_list = dp.strip().split(\"\\t\")\n",
    "    if len(ero_list) == 0:\n",
    "        continue\n",
    "        # all_eros.append(ero_list)\n",
    "    all_eros.append(ero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 1/3 of data to remove 1 triple\n",
    "remove_1 = random.sample(all_eros, int(len(all_eros) / 3))\n",
    "rem_data = [x for x in all_eros if x not in remove_1]\n",
    "# randomly select 1/2 of remaining data to remove 2 triple\n",
    "remove_2 = random.sample(rem_data, int(len(rem_data) / 2))\n",
    "rem_data = [x for x in rem_data if x not in remove_2]\n",
    "remove_3 = rem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_points = []\n",
    "for id, dp in enumerate(remove_1):\n",
    "    ero_list = dp\n",
    "    if len(ero_list) <= 1:\n",
    "        continue\n",
    "    for i in range(1): \n",
    "        chosen_index = np.random.randint(0, len(ero_list))\n",
    "        target_ero = ero_list[chosen_index]\n",
    "        context_ero = ero_list[:chosen_index] + ero_list[chosen_index + 1:]\n",
    "        prediction_points.append([context_ero, target_ero, 1])\n",
    "        if random.uniform(0,1) < 0.50:\n",
    "            e = random.sample(objects, 1)[0]\n",
    "            r = random.sample(relations, 1)[0]\n",
    "            o = random.sample(objects, 1)[0]\n",
    "            target_ero = \",\".join([e, r, o])\n",
    "            if (isinstance(target_ero, str) == False):\n",
    "                print(\"grave error\")\n",
    "            prediction_points.append([context_ero, target_ero, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_points, open(\"val_preds_1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_points = []\n",
    "for id, dp in enumerate(remove_2):\n",
    "    ero_list = dp\n",
    "    if len(ero_list) <= 2:\n",
    "        continue\n",
    "    for i in range(1): \n",
    "        chosen_idxs = np.random.randint(0, len(ero_list), 2)\n",
    "        context_ero = [x for id, x in enumerate(ero_list) if id not in chosen_idxs]\n",
    "        for j in range(2):\n",
    "            prediction_points.append([context_ero, ero_list[chosen_idxs[j]], 1])\n",
    "        # prediction_points.append([context_ero, ero_list[chosen_idxs[0]], 1])\n",
    "        # prediction_points.append([context_ero, ero_list[chosen_idxs[1]], 1])\n",
    "            if random.uniform(0,1) < 0.50:\n",
    "                e = random.sample(objects, 1)[0]\n",
    "                r = random.sample(relations, 1)[0]\n",
    "                o = random.sample(objects, 1)[0]\n",
    "                target_ero = \",\".join([e, r, o])\n",
    "                if (isinstance(target_ero, str) == False):\n",
    "                    print(\"grave error\")\n",
    "                prediction_points.append([context_ero, target_ero, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_points, open(\"val_preds_2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_points = []\n",
    "for id, dp in enumerate(remove_3):\n",
    "    ero_list = dp\n",
    "    if len(ero_list) <= 3:\n",
    "        continue\n",
    "    for i in range(1): \n",
    "        chosen_idxs = np.random.randint(0, len(ero_list), 3)\n",
    "        context_ero = [x for id, x in enumerate(ero_list) if id not in chosen_idxs]\n",
    "        for j in range(3):\n",
    "            prediction_points.append([context_ero, ero_list[chosen_idxs[j]], 1])\n",
    "            if random.uniform(0,1) < 0.50:\n",
    "                e = random.sample(objects, 1)[0]\n",
    "                r = random.sample(relations, 1)[0]\n",
    "                o = random.sample(objects, 1)[0]\n",
    "                target_ero = \",\".join([e, r, o])\n",
    "                if (isinstance(target_ero, str) == False):\n",
    "                    print(\"grave error\")\n",
    "                prediction_points.append([context_ero, target_ero, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_points, open(\"val_preds_3.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_points = []\n",
    "for id, dp in enumerate(remove_3):\n",
    "    ero_list = dp\n",
    "    if len(ero_list) <= 6:\n",
    "        continue\n",
    "    for i in range(1): \n",
    "        chosen_idxs = np.random.randint(0, len(ero_list), 5)\n",
    "        context_ero = [x for id, x in enumerate(ero_list) if id not in chosen_idxs]\n",
    "        for j in range(5):\n",
    "            prediction_points.append([context_ero, ero_list[chosen_idxs[j]], 1])\n",
    "            if random.uniform(0,1) < 0.50:\n",
    "                e = random.sample(objects, 1)[0]\n",
    "                r = random.sample(relations, 1)[0]\n",
    "                o = random.sample(objects, 1)[0]\n",
    "                target_ero = \",\".join([e, r, o])\n",
    "                if (isinstance(target_ero, str) == False):\n",
    "                    print(\"grave error\")\n",
    "                prediction_points.append([context_ero, target_ero, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_points, open(\"val_preds_5.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13639"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VG-WSDN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../top_150_50_new/train.json\", \"r\"))\n",
    "obj_map = json.load(open(\"../top_150_50_new/dict.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_word_map = json.load(open(\"top_150_50_new/categories.json\", \"r\"))[\"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "all_objects = []\n",
    "all_relations = []\n",
    "for x in data:\n",
    "    objects = x[\"objects\"]\n",
    "    triples = [[objects[r_dict[\"sub_id\"]][\"class\"], r_dict[\"predicate\"], objects[r_dict[\"obj_id\"]][\"class\"]] for r_dict in  x[\"relationships\"]]\n",
    "    \n",
    "    #replace be_on with on and be_in to in and rid to ride in the predicates in the triples\n",
    "\n",
    "    for i in range(len(triples)):\n",
    "        if triples[i][1] == \"be_on\":\n",
    "            triples[i][1] = \"on\"\n",
    "        if triples[i][1] == \"be_in\":\n",
    "            triples[i][1] = \"in\"\n",
    "        if triples[i][1] == \"rid\":\n",
    "            triples[i][1] = \"ride\"\n",
    "\n",
    "    all_objects+=[x[0] for x in triples] + [x[2] for x in triples]\n",
    "    # all_objects+= []\n",
    "    # if \"rid\" in [x[1] for x in triples]:\n",
    "    #     print(triples)\n",
    "    all_relations += [x[1] for x in triples]\n",
    "    chosen_idx = np.random.randint(0, len(triples))\n",
    "    label = \",\".join(triples[chosen_idx][1:])\n",
    "    triples[chosen_idx] = [triples[chosen_idx][0]]\n",
    "    sg = [\",\".join(x) for x in triples]\n",
    "    final_data.append([sg, label])\n",
    "    # print(sg, label)\n",
    "    # break\n",
    "\n",
    "# json.dump(final_data, open(\"../top_150_50_new/ft_train.json\", \"w\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(list(set(all_objects)), open(\"../top_150_50_new/objects.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../top_150_50_new/test.json\", \"r\"))\n",
    "obj_map = json.load(open(\"../top_150_50_new/dict.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "all_objects = []\n",
    "all_relations = []\n",
    "bb_data = {}\n",
    "paths = []\n",
    "for x in data:\n",
    "    objects = x[\"objects\"]\n",
    "    triples = [[objects[r_dict[\"sub_id\"]][\"class\"], r_dict[\"predicate\"], objects[r_dict[\"obj_id\"]][\"class\"]] for r_dict in  x[\"relationships\"]]\n",
    "\n",
    "    bb_data[x[\"path\"]] = {\"boxes\": x[\"objects\"], \"height\": x[\"height\"], \"width\": x[\"width\"], \"GT_scene\": x[\"relationships\"]}\n",
    "    \n",
    "    #replace be_on with on and be_in to in and rid to ride in the predicates in the triples\n",
    "\n",
    "    for i in range(len(triples)):\n",
    "        if triples[i][1] == \"be_on\":\n",
    "            triples[i][1] = \"on\"\n",
    "        if triples[i][1] == \"be_in\":\n",
    "            triples[i][1] = \"in\"\n",
    "        if triples[i][1] == \"rid\":\n",
    "            triples[i][1] = \"ride\"\n",
    "\n",
    "    all_objects+=[x[0] for x in triples] + [x[2] for x in triples]\n",
    "    # all_objects+= []\n",
    "    # if \"rid\" in [x[1] for x in triples]:\n",
    "    #     print(triples)\n",
    "    all_relations += [x[1] for x in triples]\n",
    "    chosen_idx = np.random.randint(0, len(triples))\n",
    "    label = \",\".join(triples[chosen_idx][1:])\n",
    "    triples[chosen_idx] = [triples[chosen_idx][0]]\n",
    "    sg = [\",\".join(x) for x in triples]\n",
    "    path = x[\"path\"]\n",
    "    paths.append(path)\n",
    "    final_data.append([sg, label, path])\n",
    "    # print(sg, label)\n",
    "    # break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json.dump(final_data, open(\"../top_150_50_new/ft_test.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json.dump(bb_data, open(\"../top_150_50_new/test_bb_data.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2c14c5f2a3b21e6c2412c8196f5145870350e81c0b737cae3e5c60eb1e1eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
